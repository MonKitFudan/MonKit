# MonKit
**MonkeyMonitorKit (MonKit) is a toolbox for the automatic action recognition, posture estimation and identification of fine motor activities in the monkeys.**

## Dataset
### MiL Dataset
The below picture is examples of The MiL Dataset. Videos have been recognized as corresponding actions and postures (labels). (a-j) Ten categories of actions have been shown: (a) Climb, (b) Hang, (c) Turn, (d) Walk, (e) Shake, (f) Jump, (g) Move Down, (h) Lie Down, (i) Stand Up and (j) Sit Down. (k-m) Three categories of postures: (k) Stand, (l) Sit and (m) Huddle. Each row represents the non-contiguous frames randomly sampled in the corresponding video. The length of all videos is from 20 to 110 frames.
![MiL_dataset](/images/MiL_dataset.jpg)

### MiL2D Dataset
The below picture is illustration of MiL2D dataset with 15 keypoints of skeleton. (a) The detailed description of the definition and location of the 15 bone points. 0, right ankle; 1 right knee; 2, left knee; 3, left ankle; 4, hip; 5, tail; 6, chin; 7, head top; 8, right wrist; 9, right elbow; 10, right shoulder; 11, left shoulder; 12, left elbow; 13, left wrist; 14, neck.
![MiL2D_dataset](/images/MiL2D_dataset.jpg)

## Installation: How to install MonKit
The configuration MonKit documentation is divided into two parts. Action_recognition refer to [action_recognition_doc.md](https://github.com/MonKitFudan/MonKit/blob/main/action_recognition_doc.md). Posture estimation refer to [posture_estimation_doc.md](https://github.com/MonKitFudan/MonKit/blob/main/posture_estimation_doc.md).
